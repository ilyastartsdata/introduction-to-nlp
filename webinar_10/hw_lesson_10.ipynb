{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw_lesson_10.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Задание\n",
        "\n",
        "Разобраться с моделькой перевода как она устроена\n",
        "запустить для перевода с русского на английский (при желании можно взять другие пары языков) два варианта с вниманием и без внимания\n",
        "оценить качество насколько корректно переводит (для теста отобрать примеры с увеличением длины текста) (так как оценка визуальная достаточно 20-ти примеров в тестовой выборке)\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "CNMOgP3Ri4n3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZPUNIxeEhIMS"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://www.manythings.org/anki/rus-eng.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWL14ER9jsm1",
        "outputId": "b6f2cc03-2c48-4332-9354-22b28fdd2552"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-07-30 14:42:44--  http://www.manythings.org/anki/rus-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
            "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14819554 (14M) [application/zip]\n",
            "Saving to: ‘rus-eng.zip’\n",
            "\n",
            "rus-eng.zip         100%[===================>]  14.13M  5.98MB/s    in 2.4s    \n",
            "\n",
            "2022-07-30 14:42:47 (5.98 MB/s) - ‘rus-eng.zip’ saved [14819554/14819554]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip rus-eng.zip -d rus-eng/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaFMj5pqjzvh",
        "outputId": "cd4caf04-ea87-40a6-b233-19bb4beadb64"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  rus-eng.zip\n",
            "  inflating: rus-eng/rus.txt         \n",
            "  inflating: rus-eng/_about.txt      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/rus-eng/ -lah"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeS-6V4mj4Yz",
        "outputId": "ba9b72f3-3886-418b-ea89-b020ea61cc1b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 71M\n",
            "drwxr-xr-x 2 root root 4.0K Jul 30 14:43 .\n",
            "drwxr-xr-x 1 root root 4.0K Jul 30 14:43 ..\n",
            "-rw-r--r-- 1 root root 1.5K May  2 01:29 _about.txt\n",
            "-rw-r--r-- 1 root root  71M May  2 01:29 rus.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = \"/content/rus-eng/rus.txt\""
      ],
      "metadata": {
        "id": "2Yzu99cZj7qc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sentence(w):\n",
        "  w = w.lower().strip()\n",
        "\n",
        "  # creating a space between a word and the punctuation following it\n",
        "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "  # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "  w = re.sub(r\"([?.!,])\", r\" \\1 \", w)\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "  w = re.sub(r\"[^a-zA-Zа-яА-Я?.!,']+\", \" \", w)\n",
        "\n",
        "  w = w.strip()\n",
        "\n",
        "  # adding a start and an end token to the sentence\n",
        "  # so that the model know when to start and stop predicting.\n",
        "  w = '<start> ' + w + ' <end>'\n",
        "  return w"
      ],
      "metadata": {
        "id": "8JTjPwEOkSbZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_sentence(\"I can't go.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Vex-8YqEkVx4",
        "outputId": "9efd0619-0186-4fac-9331-5d1dde7a0168"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<start> i can't go . <end>\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Remove the accents\n",
        "# 2. Clean the sentences\n",
        "# 3. Return word pairs in the format: [ENG, RUS]\n",
        "def create_dataset(path, num_examples):\n",
        "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "  word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')[:2]]  for l in lines[:num_examples]]\n",
        "\n",
        "  return zip(*word_pairs)"
      ],
      "metadata": {
        "id": "d2dKS5pDkYGI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en, ru = create_dataset(path_to_file, None)\n",
        "print(en[0])\n",
        "print(ru[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oj7Nq-dhkjZw",
        "outputId": "18d54715-ef53-40e3-9cc5-84ed58b9efef"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<start> go . <end>\n",
            "<start> марш ! <end>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "\n",
        "  return tensor, lang_tokenizer"
      ],
      "metadata": {
        "id": "JSoT26iAklLd"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(path, num_examples=None):\n",
        "  # creating cleaned input, output pairs\n",
        "  targ_lang, inp_lang = create_dataset(path, num_examples)\n",
        "\n",
        "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "\n",
        "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ],
      "metadata": {
        "id": "aXSwXjSnkv_B"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "BOTKg1Fsk5X9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Try experimenting with the size of that dataset\n",
        "num_examples = 100000\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
      ],
      "metadata": {
        "id": "6dCi_J61kxmo"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "# Show length\n",
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Wq6FFSnk7_L",
        "outputId": "9bc3f938-4c6b-47fe-bc12-9174743add6e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "80000 80000 20000 20000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))"
      ],
      "metadata": {
        "id": "Mb2ojB87k9Pi"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"Input Language; index to word mapping\")\n",
        "convert(inp_lang, input_tensor_train[0])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(targ_lang, target_tensor_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ra7lP4t4lGGi",
        "outputId": "04c43078-a235-430e-9706-ba184334cda6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Language; index to word mapping\n",
            "1 ----> <start>\n",
            "4 ----> я\n",
            "3896 ----> терпеливый\n",
            "186 ----> человек\n",
            "3 ----> .\n",
            "2 ----> <end>\n",
            "\n",
            "Target Language; index to word mapping\n",
            "1 ----> <start>\n",
            "13 ----> i'm\n",
            "9 ----> a\n",
            "703 ----> patient\n",
            "216 ----> man\n",
            "3 ----> .\n",
            "2 ----> <end>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "5gAk7pmrlJEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 300\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "metadata": {
        "id": "jEFSSVxhlHNT"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TSNM143lLLL",
        "outputId": "971ecfef-53ba-4000-cb5e-1d8012d171c5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 15]), TensorShape([64, 11]))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=False,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    \n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "metadata": {
        "id": "tx7RQ1L_lMo5"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "# print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4x12pdElUfQ",
        "outputId": "69748ba5-ffdc-438a-c967-230a01b6bb11"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x, initial_state=hidden)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    # fc - fully connected слой\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state"
      ],
      "metadata": {
        "id": "fkvFr61YlVy_"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "decoder_sample_x, decoder_sample_h = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden)"
      ],
      "metadata": {
        "id": "_irtA04rlX69"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "8qhvrkLulaRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "# это маскированный лосс. Мы не хотим чтобы паддиноговые нулевые токены учитывались в лоссе\n",
        "# поэтому мы получаем падинговые элементы (0) - маска\n",
        "# и зануляем там лосс\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "metadata": {
        "id": "dWAKn0d-lZhk"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "vJ-ZbKzBllsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_dir = './training_nmt_checkpoints'\n",
        "\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "metadata": {
        "id": "oFl7IM3cllN3"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# функция для обучения модели\n",
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "    # inp - входная последовательность (русский)\n",
        "    # target - английский\n",
        "    # enc_hidden - вектор скрытого состояния энкодера\n",
        "    loss = 0\n",
        "    \n",
        "    # берём градиенты\n",
        "    with tf.GradientTape() as tape:\n",
        "        # передаём в энкодер входную последовательность и скрытое состояние\n",
        "        # получаем выход скрытого состояния\n",
        "        enc_hidden = encoder(inp, enc_hidden)\n",
        "        \n",
        "        # присваиваем для инициализации ветора скрытого состояния в декодере\n",
        "        dec_hidden = enc_hidden\n",
        "        # добавляем тег старта для начала перевода, берём его индекс, переводим в тензор\n",
        "        dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "        # Teacher forcing - feeding the target as the next input\n",
        "        # идём по таргету, по тому тексту на который хотим натренировать перевод\n",
        "        # начинаем не с нулевого токена, потому что первым стоит тег <start>\n",
        "        for t in range(1, targ.shape[1]):\n",
        "            # passing enc_output to the decoder\n",
        "            # мы не хотим затачиваться на предикт, прокидывая градиент будем улучшать предикт\n",
        "            predictions, dec_hidden = decoder(dec_input, dec_hidden)\n",
        "            # вычисляем лосс-функцию\n",
        "            loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "            # using teacher forcing\n",
        "            # записываем текущий токен (таргет) на вход декодера - получается смещение\n",
        "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "        batch_loss = (loss / int(targ.shape[1]))\n",
        "        # формируем все переменные по которым будет браться градиент\n",
        "        variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "        # берём градиент и применяем его к лосс-функции\n",
        "        gradients = tape.gradient(loss, variables)\n",
        "\n",
        "        optimizer.apply_gradients(zip(gradients, variables))\n",
        "    # возвращаем батч-лосс\n",
        "    return batch_loss"
      ],
      "metadata": {
        "id": "KZBVrGCOloHs"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 50\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 999
        },
        "id": "Y1vMlvUOlqGo",
        "outputId": "7add5c60-4eb0-477a-f8eb-caccb0f03190"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-25-f884ceb8552b>\", line 10, in <module>\n",
            "    batch_loss = train_step(inp, targ, enc_hidden)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\", line 915, in __call__\n",
            "    result = self._call(*args, **kwds)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\", line 947, in _call\n",
            "    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\", line 2957, in __call__\n",
            "    filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\", line 1854, in _call_flat\n",
            "    ctx, args, cancellation_manager=cancellation_manager))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\", line 504, in call\n",
            "    ctx=ctx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\", line 55, in quick_execute\n",
            "    inputs, attrs, num_outputs)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
            "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 732, in getmodule\n",
            "    for modname, module in sys.modules.copy().items():\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "etu14jRolvxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# функция для оценки перевода\n",
        "def evaluate(sentence):\n",
        "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "    # препроцессинг предложения\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "    # переводим каждый токен в индекс\n",
        "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_inp,\n",
        "                                                         padding='post')\n",
        "    # переводим в тензор\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    result = ''\n",
        "\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    # посылаем в энкодер\n",
        "    enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden = decoder(dec_input, dec_hidden)\n",
        "\n",
        "        # storing the attention weights to plot later on\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "        # т.к. у нас нет таргета - передаём предсказание на следующий шаг\n",
        "        result += targ_lang.index_word[predicted_id] + ' '\n",
        "        # останавливаемся если встречаем токен конца\n",
        "        if targ_lang.index_word[predicted_id] == '<end>':\n",
        "            return result, sentence\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, sentence"
      ],
      "metadata": {
        "id": "dsy1eBQYlsBn"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(sentence):\n",
        "  result, sentence = evaluate(sentence)\n",
        "\n",
        "  print('Input: %s' % (sentence))\n",
        "  print('Predicted translation: {}'.format(result))"
      ],
      "metadata": {
        "id": "92XrTcXiZ-ow"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "92BCndtQaBoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxgRjzmgaAMZ",
        "outputId": "ec3e9fcd-b206-48e6-e9e1-52902dbbf75c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f0df06e2ad0>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translate('Здесь хорошо.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGBsq-IuaEN8",
        "outputId": "c266b3bd-88ae-45a9-deb3-6f8c128bce36"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <start> здесь хорошо . <end>\n",
            "Predicted translation: it's is here . . . . . . . . \n"
          ]
        }
      ]
    }
  ]
}